{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0-mkxd1ze3-4yqkd8",
      "metadata": {},
      "source": [
        "# Visualization of Loss Landscapes\n",
        "\n",
        "This notebook explores the loss landscape of neural networks by training multiple networks initialized with different random weights on the same dataset. We'll visualize how different initializations lead to different trajectories through the loss landscape, but often converge to similar loss values.\n",
        "\n",
        "The experiment involves:\n",
        "\n",
        "- Creating a simple synthetic dataset\n",
        "- Initializing multiple neural networks with different random seeds\n",
        "- Training each network with identical hyperparameters and optimizer settings\n",
        "- Tracking the loss trajectory for each network\n",
        "- Visualizing the loss landscapes in 2D using PCA projection of weight space\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-1-mkxd1ze3-fpxmjr",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_moons\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-2-mkxd1ze3-9iqiss",
      "metadata": {},
      "source": [
        "## 1. Create Synthetic Dataset\n",
        "\n",
        "We'll use a simple 2D moons dataset for binary classification. This provides a non-linear decision boundary that requires a neural network to learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-3-mkxd1ze3-8ugmbh",
      "metadata": {},
      "source": [
        "# Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X)\n",
        "y_tensor = torch.FloatTensor(y).unsqueeze(1)\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)  # Fixed order\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0', alpha=0.6)\n",
        "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1', alpha=0.6)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Synthetic Dataset: Moons')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-4-mkxd1ze3-b7ov4j",
      "metadata": {},
      "source": [
        "## 2. Define Neural Network Architecture\n",
        "\n",
        "We'll use a simple feedforward neural network with two hidden layers. The architecture is kept simple to make the loss landscape visualization more interpretable.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-5-mkxd1ze3-l75nic",
      "metadata": {},
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 8)\n",
        "        self.fc2 = nn.Linear(8, 4)\n",
        "        self.fc3 = nn.Linear(4, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "    \n",
        "    def get_weights_flat(self):\n",
        "        \"\"\"Flatten all weights into a single vector\"\"\"\n",
        "        return torch.cat([p.data.flatten() for p in self.parameters()])\n",
        "\n",
        "# Test the model\n",
        "test_model = SimpleNN()\n",
        "print(f\"Model architecture:\\n{test_model}\")\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in test_model.parameters())}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-6-mkxd1ze3-lgykwr",
      "metadata": {},
      "source": [
        "## 3. Training Function\n",
        "\n",
        "Define a training function that will be used consistently across all random initializations. We'll track the loss at each epoch and the weight vectors for visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-7-mkxd1ze3-rcjlwa",
      "metadata": {},
      "source": [
        "def train_network(model, dataloader, epochs=100, lr=0.1):\n",
        "    \"\"\"Train a network and return loss history and weight trajectory\"\"\"\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    \n",
        "    loss_history = []\n",
        "    weight_trajectory = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        loss_history.append(avg_loss)\n",
        "        \n",
        "        # Store weight snapshot every 5 epochs\n",
        "        if epoch % 5 == 0:\n",
        "            weight_trajectory.append(model.get_weights_flat().numpy())\n",
        "    \n",
        "    return loss_history, weight_trajectory\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-8-mkxd1ze3-tn5ueo",
      "metadata": {},
      "source": [
        "## 4. Train Multiple Networks with Different Initializations\n",
        "\n",
        "Now we'll train multiple networks, each with a different random initialization but identical training procedure. This allows us to explore how initialization affects the optimization trajectory.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-9-mkxd1ze3-ajhm6i",
      "metadata": {},
      "source": [
        "# Training parameters\n",
        "num_networks = 10\n",
        "epochs = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Store results\n",
        "all_loss_histories = []\n",
        "all_weight_trajectories = []\n",
        "\n",
        "print(\"Training networks with different random initializations...\\n\")\n",
        "\n",
        "for i in range(num_networks):\n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(i * 100)\n",
        "    \n",
        "    # Initialize new network\n",
        "    model = SimpleNN()\n",
        "    \n",
        "    # Train the network\n",
        "    loss_history, weight_trajectory = train_network(model, dataloader, epochs, learning_rate)\n",
        "    \n",
        "    all_loss_histories.append(loss_history)\n",
        "    all_weight_trajectories.append(weight_trajectory)\n",
        "    \n",
        "    print(f\"Network {i+1}/{num_networks} - Final Loss: {loss_history[-1]:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-10-mkxd1ze3-8yye05",
      "metadata": {},
      "source": [
        "## 5. Visualize Loss Trajectories\n",
        "\n",
        "First, let's visualize how the loss evolves over training for each network. Despite different initializations, we expect convergence to similar loss values.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-11-mkxd1ze3-bt31dz",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i, loss_history in enumerate(all_loss_histories):\n",
        "    plt.plot(loss_history, alpha=0.7, label=f'Network {i+1}')\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (BCE)', fontsize=12)\n",
        "plt.title('Loss Trajectories for Different Random Initializations', fontsize=14)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "final_losses = [history[-1] for history in all_loss_histories]\n",
        "print(f\"\\nFinal Loss Statistics:\")\n",
        "print(f\"Mean: {np.mean(final_losses):.4f}\")\n",
        "print(f\"Std:  {np.std(final_losses):.4f}\")\n",
        "print(f\"Min:  {np.min(final_losses):.4f}\")\n",
        "print(f\"Max:  {np.max(final_losses):.4f}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-12-mkxd1ze3-3c1tks",
      "metadata": {},
      "source": [
        "## 6. Visualize Weight Space Trajectories with PCA\n",
        "\n",
        "To visualize the high-dimensional weight space, we'll use PCA to project the weight trajectories into 2D. This shows how different initializations lead to different paths through weight space.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-13-mkxd1ze3-u3zl0e",
      "metadata": {},
      "source": [
        "# Flatten all weight trajectories into a single array for PCA\n",
        "all_weights = []\n",
        "trajectory_indices = []\n",
        "\n",
        "for i, trajectory in enumerate(all_weight_trajectories):\n",
        "    all_weights.extend(trajectory)\n",
        "    trajectory_indices.extend([i] * len(trajectory))\n",
        "\n",
        "all_weights = np.array(all_weights)\n",
        "trajectory_indices = np.array(trajectory_indices)\n",
        "\n",
        "# Apply PCA to reduce to 2D\n",
        "pca = PCA(n_components=2)\n",
        "weights_2d = pca.fit_transform(all_weights)\n",
        "\n",
        "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cell-14-mkxd1ze3-froej2",
      "metadata": {},
      "source": [
        "# Visualize trajectories in 2D PCA space\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, num_networks))\n",
        "\n",
        "for i in range(num_networks):\n",
        "    # Get indices for this trajectory\n",
        "    mask = trajectory_indices == i\n",
        "    traj_2d = weights_2d[mask]\n",
        "    \n",
        "    # Plot trajectory\n",
        "    plt.plot(traj_2d[:, 0], traj_2d[:, 1], 'o-', color=colors[i], \n",
        "             alpha=0.6, linewidth=2, markersize=4, label=f'Network {i+1}')\n",
        "    \n",
        "    # Mark start and end points\n",
        "    plt.scatter(traj_2d[0, 0], traj_2d[0, 1], color=colors[i], \n",
        "                s=200, marker='*', edgecolors='black', linewidths=2, zorder=5)\n",
        "    plt.scatter(traj_2d[-1, 0], traj_2d[-1, 1], color=colors[i], \n",
        "                s=200, marker='s', edgecolors='black', linewidths=2, zorder=5)\n",
        "\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
        "plt.title('Weight Space Trajectories (PCA Projection)\\n* = Start, ■ = End', fontsize=14)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-15-mkxd1ze3-anztuk",
      "metadata": {},
      "source": [
        "## 7. Create Loss Landscape Heatmap\n",
        "\n",
        "Finally, we'll create a 2D loss landscape by evaluating the loss at a grid of points in the PCA-projected weight space. This gives us a visual representation of the loss surface.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-16-mkxd1ze3-eie34x",
      "metadata": {},
      "source": [
        "def evaluate_loss_at_weights(weight_vector, X_tensor, y_tensor):\n",
        "    \"\"\"Evaluate loss for a given weight vector\"\"\"\n",
        "    model = SimpleNN()\n",
        "    \n",
        "    # Reconstruct model weights from flat vector\n",
        "    idx = 0\n",
        "    for param in model.parameters():\n",
        "        param_length = param.numel()\n",
        "        param.data = torch.FloatTensor(weight_vector[idx:idx+param_length]).reshape(param.shape)\n",
        "        idx += param_length\n",
        "    \n",
        "    # Evaluate loss\n",
        "    criterion = nn.BCELoss()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_tensor)\n",
        "        loss = criterion(outputs, y_tensor)\n",
        "    \n",
        "    return loss.item()\n",
        "\n",
        "# Create a grid in PCA space\n",
        "resolution = 30\n",
        "x_min, x_max = weights_2d[:, 0].min() - 1, weights_2d[:, 0].max() + 1\n",
        "y_min, y_max = weights_2d[:, 1].min() - 1, weights_2d[:, 1].max() + 1\n",
        "\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n",
        "                     np.linspace(y_min, y_max, resolution))\n",
        "\n",
        "# Evaluate loss at each grid point\n",
        "print(\"Computing loss landscape (this may take a moment)...\")\n",
        "loss_landscape = np.zeros_like(xx)\n",
        "\n",
        "for i in range(resolution):\n",
        "    for j in range(resolution):\n",
        "        # Project 2D point back to high-dimensional weight space\n",
        "        pca_coords = np.array([xx[i, j], yy[i, j]])\n",
        "        weight_vector = pca.inverse_transform(pca_coords)\n",
        "        \n",
        "        # Evaluate loss\n",
        "        loss_landscape[i, j] = evaluate_loss_at_weights(weight_vector, X_tensor, y_tensor)\n",
        "    \n",
        "    if (i + 1) % 5 == 0:\n",
        "        print(f\"Progress: {(i+1)/resolution*100:.0f}%\")\n",
        "\n",
        "print(\"Loss landscape computation complete!\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cell-17-mkxd1ze3-pn8cfn",
      "metadata": {},
      "source": [
        "# Visualize the loss landscape with trajectories\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Plot loss landscape as contour\n",
        "contour = plt.contourf(xx, yy, loss_landscape, levels=20, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(contour, label='Loss (BCE)')\n",
        "\n",
        "# Overlay trajectories\n",
        "for i in range(num_networks):\n",
        "    mask = trajectory_indices == i\n",
        "    traj_2d = weights_2d[mask]\n",
        "    plt.plot(traj_2d[:, 0], traj_2d[:, 1], 'o-', color='white', \n",
        "             alpha=0.8, linewidth=2, markersize=3)\n",
        "    plt.scatter(traj_2d[0, 0], traj_2d[0, 1], color='red', \n",
        "                s=150, marker='*', edgecolors='white', linewidths=2, zorder=5)\n",
        "    plt.scatter(traj_2d[-1, 0], traj_2d[-1, 1], color='lime', \n",
        "                s=150, marker='s', edgecolors='white', linewidths=2, zorder=5)\n",
        "\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
        "plt.title('Loss Landscape with Training Trajectories\\n* = Start (red), ■ = End (green)', fontsize=14)\n",
        "plt.grid(True, alpha=0.3, color='white')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-18-mkxd1ze3-j115zy",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how to visualize loss landscapes by training multiple neural networks with different random initializations on the same dataset. Key observations:\n",
        "\n",
        "1. Different random initializations lead to different starting points in weight space\n",
        "2. Despite different trajectories, networks tend to converge to similar loss values\n",
        "3. The PCA projection reveals that optimization paths can vary significantly even with identical training procedures\n",
        "4. The loss landscape shows the complex, non-convex nature of neural network optimization\n",
        "5. All networks successfully navigate from their random starting points to low-loss regions\n",
        "\n",
        "This visualization technique helps us understand the geometry of neural network optimization and why different initializations can lead to different solutions with similar performance. The loss landscape perspective provides valuable insights into the training dynamics of deep learning models.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}